{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import des librairies nécessaires.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai fait le choix d'utiliser un modèle dont je ferai le finetuning.  C'est basé sur un CNN, donc très bien pour faire de l'analyse d'images et en particulier de la détection d'élements dans notre cas. : https://pytorch.org/vision/master/models/faster_rcnn.html\n",
    "J'ai conscience que d'autres modèles existent, mais dans mon cas j'ai choisi le modèle le plus simple par soucis de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les boxes ont été crées avec labelimg. Le format des coordonées est celui de YOLO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des Labels et des images \n",
    "import glob\n",
    "\n",
    "data = []\n",
    "\n",
    "# Get train labels\n",
    "for file in glob.glob('imagesfull/labels/*.txt'):\n",
    "    with open(file, 'r') as f:\n",
    "        content = f.read()\n",
    "        content = content.split(' ')\n",
    "        if content[1]>content[3]:\n",
    "            content[1], content[3] = content[3], content[1]\n",
    "        if content[2]>content[4]:\n",
    "            content[2], content[4] = content[4], content[2]\n",
    "        \n",
    "        filename = os.path.splitext(os.path.basename(file))[0]\n",
    "        row = {'Image': filename, 'x1': content[1], 'y1': content[2], 'w': content[3], 'h': content[4].strip()}\n",
    "        data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "#Conversion des coordonées format  Yolo Coordinates en coordoénes x1, y1, x2, y2 (format requis pour notre modèle) \n",
    "df[\"x1\"] = pd.to_numeric(df[\"x1\"])\n",
    "df[\"y1\"] = pd.to_numeric(df[\"y1\"])\n",
    "df[\"w\"] = pd.to_numeric(df[\"w\"])\n",
    "df[\"h\"] = pd.to_numeric(df[\"h\"])\n",
    "\n",
    "df[\"x2\"] = df[\"x1\"] + df[\"w\"]/2\n",
    "df[\"y2\"] = df[\"y1\"] + df[\"h\"]/2\n",
    "\n",
    "df[\"x1\"] = df[\"x1\"] - df[\"w\"]/2\n",
    "df[\"y1\"] = df[\"y1\"] - df[\"h\"]/2\n",
    "\n",
    "df.drop(columns=[\"w\", \"h\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Image        x1        y1  \\\n",
      "0  61055990_PHO_210115102717_36037262_BE578ZW_472...  0.096250  0.038334   \n",
      "1  61098159_PHO_210621114136_36675117_DS228RZ_488...  0.187500  0.048334   \n",
      "2  61113096_PHO_210906112642_37013023_BR381KB_496...  0.096355  0.115234   \n",
      "3  61634454_PHO_210614153649_36641416_CW488EY_487...  0.106250  0.066667   \n",
      "4  61635037_PHO_210617165130_36663833_EC496EA_488...  0.131250  0.001667   \n",
      "\n",
      "         x2        y2  \n",
      "0  0.616250  0.800001  \n",
      "1  0.627500  0.770000  \n",
      "2  0.675782  0.751953  \n",
      "3  0.626250  0.743334  \n",
      "4  0.785000  0.801667  \n",
      "61055990_PHO_210115102717_36037262_BE578ZW_47242223\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(df['Image'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 600\n",
      "105.0 1.0002 628.0 481.0002\n"
     ]
    }
   ],
   "source": [
    "#Test to draw a rectangle on the image\n",
    "\n",
    "im= Image.open('imagesfull/bis/images/'+df['Image'][3]+'.jpg')\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "im_width, im_height = im.size\n",
    "print(im.width, im.height)\n",
    "\n",
    "x1 = 0.131250 * im_width\n",
    "y1 = 0.001667 * im_height\n",
    "x2 =  0.785000 * im_width\n",
    "y2 = 0.801667*im_height\n",
    "\n",
    "print(x1, y1, x2, y2)\n",
    "\n",
    "draw.rectangle((x1, y1, x2, y2), outline='red')\n",
    "\n",
    "# afficher l'image avec le rectangle dessiné\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['61055990_PHO_210115102717_36037262_BE578ZW_47242223'\n",
      " '61098159_PHO_210621114136_36675117_DS228RZ_488599412'\n",
      " '61113096_PHO_210906112642_37013023_BR381KB_496293520'\n",
      " '61634454_PHO_210614153649_36641416_CW488EY_48781857'\n",
      " '61635037_PHO_210617165130_36663833_EC496EA_488359414'\n",
      " '61637276_PHO_210705070511_36747110_FB170MK_490197018'\n",
      " '61638565_PHO_210719090259_36806831_CT701HG_491640312'\n",
      " '61865076_PHO_210521163156_36539817_FT857VF_485319017'\n",
      " '61865102_PHO_210531182130_36574497_FW453LE_486177017'\n",
      " '61876904_PHO_210723090635_36832542_CP966ZT_492277515'\n",
      " '62474067_PHO_210504101605_6105-851839482_DM-867-AT_fc2d6d93-f243-4825-8974-a52d520718c2'\n",
      " '62489478_PHO_210706183211_36758355_EZ044ZM_490533616'\n",
      " '62490594_PHO_210709111201_36774055_CJ431ZX_49090029' '911581721'\n",
      " '97705979_PHO_210514093024_36508533_AP832PZ_484531714'\n",
      " '97725165_PHO_210525084056_36544016_BY775AY_485388016'\n",
      " '97735324_PHO_210604160140_36599821_FM948RX_486806814'\n",
      " '97764167_PHO_210902093553_855952396_BA-252-ZK_1519418571'\n",
      " '97765851_PHO_210607182606_36607799_EP334KJ_487004317'\n",
      " '97799703_PHO_210618165614_36670045_CQ832JQ_488507523'\n",
      " '97811283_PHO_210623115134_36692579_DA949HS_488959322'\n",
      " '97812626_PHO_210623154633_36694497_EH576YB_489012614'\n",
      " '97832249_PHO_210630154638_36730557_EB290ZZ_489823217'\n",
      " '97841320_PHO_210705102243_36747954_EM015AR_490227532'\n",
      " '97876528_PHO_210720103634_36814849_AA955GB_491846213'\n",
      " '97879349_PHO_210726175203_36843020_CY722YJ_492528011'\n",
      " '97884723_PHO_210902152219_37001443_FR515HG_496018114'\n",
      " '97893635_PHO_210803163105_36879513_AD035EQ_493362818'\n",
      " '97903315_PHO_210811142531_36911437_CR370CM_49398948'\n",
      " '97913027_PHO_210812103537_36914731_DD643XS_494052612'\n",
      " '97935394_PHO_210816080101_36923650_EL551CA_494206117'\n",
      " '97951000_PHO_210823152717_36951712_CF639WH_494772513'\n",
      " '97965206_PHO_210830111603_36979188_CV608BS_495441011'\n",
      " '97968167_PHO_210831101122_36985573_FJ330TJ_495589210'\n",
      " '97975639_PHO_210902115630_37000012_BN376ND_495977915'\n",
      " '97997676_PHO_210910170657_37043013_CB217BT_49708293'\n",
      " '98003504_PHO_210913183623_37050406_FW927AM_497280517'\n",
      " '98066505_PHO_211007185202_37174392_FD446ZN_50034907'\n",
      " '98075586_PHO_211011165456_37186653_EW245KL_50065202'\n",
      " '98076813_PHO_211013102837_5665-868641294_BG-231-XX_4dadb45a-ad0a-4d54-aa3b-9f09c01d968f']\n"
     ]
    }
   ],
   "source": [
    "#on récupère les images uniques qui nous servirons pour l'objet datasetCustom\n",
    "unique_images = df.Image.unique()\n",
    "\n",
    "df.to_csv('labels.csv', index=False)\n",
    "print(unique_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de notre dataset customisé pour ensuite le donner à notre dataloader. \n",
    "Il faut donner notre dataframe contenant les images ainsi que les coordonées des boxes. Les images uniques serviront à les utiliser qu'une seule fois, et les indices pour lier les labels / box / images comme il faut. \n",
    "\n",
    "On retourne un tenseur compatible avec notre data loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataSet(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, unique_imgs , indices):\n",
    "        self.df = df\n",
    "        self.unique_imgs = unique_imgs\n",
    "        self.indices = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #Charger l'image\n",
    "        img_name = self.unique_imgs[self.indices[idx]]\n",
    "        boxes = self.df[self.df.Image == img_name].values[:, 1:].astype(\"float\")\n",
    "        img = Image.open(\"./imagesfull/images/\" + img_name + \".jpg\").convert(\"RGB\")\n",
    "        img = img.resize((800, 600), Image.LANCZOS)\n",
    "        \n",
    "\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "\n",
    "        return T.ToTensor()(img), target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 3, 19, 8, 24, 0, 36, 26]\n"
     ]
    }
   ],
   "source": [
    "#On sépare le jeu de données en un ensemle d'entrainement et un ensemble de test (80 / 20)\n",
    "train_inds, test_inds = train_test_split(range(unique_images.shape[0]), test_size=0.2)\n",
    "\n",
    "print(test_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la fonction custom_collate, elle peut servir à traiter les éléments en lots avant de les donner au dataloader. Dans notre cas, nous avons pré traité les données avant, c'est pour cela que elle renvoie exactement ce que on lui donne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(CustomDataSet(df, unique_images, train_inds), \n",
    "                                       batch_size=2, \n",
    "                                       shuffle=True,\n",
    "                                       collate_fn=custom_collate)\n",
    "\n",
    "val_dl = torch.utils.data.DataLoader(CustomDataSet(df, unique_images, test_inds),\n",
    "                                        batch_size=2,\n",
    "                                        shuffle=True,\n",
    "                                        collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idéalement si l'on peut on utilise un GPU pour l'entrainement autrement ce sera le CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Programmation\\IA\\TestBCA\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "f:\\Programmation\\IA\\TestBCA\\env\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  #2 classes : une pour les portes avants et une pour le background.\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) #On précise le nombre de classes pour finetuner le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilisation d'un optimiser de descente de gradient stochastique. Les paramètres sont à ajuster pour obtenir de meilleurs résultats. (De même pour les epochs)\n",
    "optimiser = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.402452134821242\n",
      "3.7715503493643014\n",
      "2.694903994624104\n",
      "1.4930469409187002\n",
      "1.4030803411622326\n"
     ]
    }
   ],
   "source": [
    "#Phase d'entrainement du modèle. On donne epoch par epoch les images et les targets pour entrainer le modèle.\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for data in train_dl:\n",
    "        imgs = []\n",
    "        targets = []\n",
    "        for d in data:\n",
    "            imgs.append(d[0].to(device))\n",
    "            targ = {}\n",
    "            targ[\"boxes\"] = torch.from_numpy(d[1][\"boxes\"]).to(device)\n",
    "            targ[\"labels\"] = d[1][\"labels\"].to(device)\n",
    "\n",
    "\n",
    "            targets.append(targ)\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(v for v in loss_dict.values())\n",
    "        epoch_loss += loss.cpu().detach().numpy()\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "    print(epoch_loss)\n",
    "    #Epoch loss nous permet de surveiller la perte d'entrainement du modèle. On s'attend à ce que cette perte diminue à chaque epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': array([[0.2396875, 0.0016665, 0.8428125, 0.9499995]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "#Debut de la phase d'inférence. On évalue le modèle sur l'ensemble de test pour voir comment il se comporte sur des données qu'il n'a jamais vu.\n",
    "model.eval()\n",
    "data = iter(val_dl).__next__()\n",
    "\n",
    "print(data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour évaluer le modèle nous faisons une inférence sur le df de val. Nous prenons 2 indicateurs qui sont : la similitude en % avec la boîte réelle, ainsi que le % de box qui sont supérieurs à un certain treshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([3.6626103e-04, 0.0000000e+00, 2.0814431e+00, 1.9141083e+00],\n",
      "      dtype=float32), array([0.00322129, 0.        , 1.6998055 , 2.298231  ], dtype=float32), array([0., 0., 0., 0.]), array([0.00413149, 0.        , 0.87406456, 1.6055486 ], dtype=float32), array([8.5678598e-04, 0.0000000e+00, 9.7575241e-01, 1.2119522e+00],\n",
      "      dtype=float32), array([0., 0., 0., 0.]), array([1.3288165, 0.       , 1.7261846, 0.1723454], dtype=float32), array([0., 0., 0., 0.])]\n",
      "[[190.5    150.0003 661.5    566.0001]\n",
      " [140.5     39.9999 463.5    455.9997]\n",
      " [191.75     0.9999 674.25   569.9997]\n",
      " [144.       0.9999 678.     452.9997]\n",
      " [ 85.      39.9999 501.     446.0001]\n",
      " [106.6664   2.2221 460.8328 443.3331]\n",
      " [ 77.      23.0001 493.     480.0003]\n",
      " [ 86.25     1.2    503.75   520.8   ]]\n",
      "En moyenne, les box prédites ont une similitude de 0.00% avec les box réelles.\n",
      "0.00% des boîtes ont une similitude supérieure à 70.00%d .\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "model.eval()\n",
    "val_targets = []\n",
    "val_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data_values in val_dl:\n",
    "        imgs = [d[0].to(device) for d in data_values]\n",
    "        targets = [d[1] for d in data_values]\n",
    "\n",
    "        # Extraire les boîtes englobantes et les étiquettes pour chaque image\n",
    "        for t in targets:\n",
    "            targ = {}\n",
    "            targ[\"boxes\"] = torch.from_numpy(t[\"boxes\"]).to(device)\n",
    "            targ[\"labels\"] = t[\"labels\"].to(device)\n",
    "            val_targets.append(targ[\"boxes\"].cpu().numpy())\n",
    "\n",
    "        # Effectuer une inférence sur le lot d'images\n",
    "        prediction = model(imgs)\n",
    "\n",
    "        # Extraire les boîtes englobantes prédites et les scores de confiance pour chaque image\n",
    "        for i in range(len(prediction)):\n",
    "            out_bbox1 = prediction[i][\"boxes\"].cpu()\n",
    "            out_scores1 = prediction[i][\"scores\"].cpu()\n",
    "\n",
    "            # On ne garde que la meilleure box\n",
    "            keep = torchvision.ops.nms(out_bbox1, out_scores1, 0.7)\n",
    "\n",
    "            if len(keep) > 0:\n",
    "                best_box1 = out_bbox1[keep[0]]\n",
    "                best_score = out_scores1[keep[0]]\n",
    "                best_box1 = prediction[i][\"boxes\"][keep[0]].cpu().numpy()\n",
    "\n",
    "                # Ajouter l'étiquette prédite à la liste val_predictions\n",
    "                val_predictions.append(best_box1)\n",
    "            else:\n",
    "                # Ajouter une étiquette négative si aucune box n'est détectée\n",
    "                val_predictions.append(np.zeros((4,)))\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    # Calculer les coordonnées de l'intersection des deux boîtes\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "\n",
    "    # Calculer l'aire de l'intersection des deux boîtes\n",
    "    area_intersection = max(0, x2 - x1 + 1) * max(0, y2 - y1 + 1)\n",
    "\n",
    "    # Calculer l'aire de chaque boîte\n",
    "    area_box1 = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
    "    area_box2 = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
    "\n",
    "    # Calculer l'IoU\n",
    "    iou = area_intersection / (area_box1 + area_box2 - area_intersection)\n",
    "\n",
    "    # Convertir l'IoU en pourcentage\n",
    "    iou_percent = iou * 100\n",
    "\n",
    "    return iou_percent\n",
    "\n",
    "val_targets = np.array(val_targets)\n",
    "\n",
    "\n",
    "val_targets = np.squeeze(val_targets)\n",
    "#resize val_targets pour que ce soit normalisé avec les prédictions.\n",
    "for i in range(len(val_targets)):\n",
    "    val_targets[i][0] = val_targets[i][0]*800\n",
    "    val_targets[i][1] = val_targets[i][1]*600\n",
    "    val_targets[i][2] = val_targets[i][2]*800\n",
    "    val_targets[i][3] = val_targets[i][3]*600\n",
    "results = []\n",
    "\n",
    "for i in range(len(val_predictions)):\n",
    "    result = compute_iou(val_targets[i], val_predictions[i])\n",
    "    results.append(result)\n",
    "\n",
    "\n",
    "results = np.array(results)\n",
    "moyenne_similitude = np.mean(results)\n",
    "\n",
    "print(\"En moyenne, les box prédites ont une similitude de {:.2f}% avec les box réelles.\".format(moyenne_similitude))\n",
    "\n",
    "\n",
    "treshold = 70\n",
    "# Calculer le pourcentage de boîtes qui ont une similitude supérieure au treshold\n",
    "nbr_boites_superieures = np.count_nonzero(results >= treshold)\n",
    "pourcentage_boites_superieures = nbr_boites_superieures / len(results) * 100\n",
    "\n",
    "print(\"{:.2f}% des boîtes ont une similitude supérieure à {:.2f}%d .\".format(pourcentage_boites_superieures, treshold))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous faison une inférence simple pour afficher l'image et la box associée (sur une nouvelle image.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On récupère les images et les targets pour les afficher\n",
    "img = data[0][0]\n",
    "boxes = data[0][1][\"boxes\"]\n",
    "labels = data[0][1][\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on donne l'image à notre modèle.\n",
    "output = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([], size=(0, 4), grad_fn=<StackBackward0>),\n",
       "  'labels': tensor([], dtype=torch.int64),\n",
       "  'scores': tensor([], grad_fn=<IndexBackward0>)}]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On recupère les résultats de la prédiction (les boites englobantes et les scores associés)\n",
    "out_bbox = output[0][\"boxes\"]\n",
    "out_scores = output[0][\"scores\"]\n",
    "\n",
    "out_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Supprossion des boites qui se chevauchent si le IoU est supérieur à 0.7\n",
    "#Je m'en sers pour trier les scores par indice décroissant\n",
    "keep = torchvision.ops.nms(out_bbox, out_scores, 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 4), grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = (img.permute(1, 2, 0).cpu().detach().numpy()*255).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[194, 176, 156],\n",
       "        [188, 170, 150],\n",
       "        [194, 176, 156],\n",
       "        ...,\n",
       "        [ 71,  59,  35],\n",
       "        [ 72,  60,  36],\n",
       "        [ 71,  59,  35]],\n",
       "\n",
       "       [[194, 176, 156],\n",
       "        [188, 170, 150],\n",
       "        [194, 176, 156],\n",
       "        ...,\n",
       "        [ 71,  59,  35],\n",
       "        [ 72,  60,  36],\n",
       "        [ 70,  58,  34]],\n",
       "\n",
       "       [[195, 177, 157],\n",
       "        [189, 171, 151],\n",
       "        [194, 176, 156],\n",
       "        ...,\n",
       "        [ 71,  59,  35],\n",
       "        [ 72,  60,  36],\n",
       "        [ 70,  58,  34]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[251, 228, 212],\n",
       "        [251, 228, 212],\n",
       "        [250, 227, 211],\n",
       "        ...,\n",
       "        [178, 142, 128],\n",
       "        [178, 142, 128],\n",
       "        [178, 142, 128]],\n",
       "\n",
       "       [[251, 228, 212],\n",
       "        [251, 228, 212],\n",
       "        [251, 228, 212],\n",
       "        ...,\n",
       "        [179, 146, 131],\n",
       "        [176, 143, 128],\n",
       "        [173, 140, 125]],\n",
       "\n",
       "       [[252, 229, 213],\n",
       "        [252, 229, 213],\n",
       "        [251, 228, 212],\n",
       "        ...,\n",
       "        [181, 148, 133],\n",
       "        [176, 143, 128],\n",
       "        [171, 138, 123]]], dtype=uint8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 800, 3)\n",
      "[0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# supposons que la forme de l'image d'origine est (hauteur, largeur, canaux)\n",
    "hauteur, largeur, canal = im.shape\n",
    "print(im.shape)\n",
    "\n",
    "# On garde seulement la boîte qui à le meilleur score. Ou si il n'y a pas de box détectée, on crée une box vide.\n",
    "try:\n",
    "    best_box = out_bbox[keep[0]].cpu().detach().numpy()\n",
    "except:\n",
    "    best_box = np.zeros((4,))\n",
    "\n",
    "# convertir les coordonnées normalisées en coordonnées entières\n",
    "best_box_int = [int(best_box[0] * largeur), int(best_box[1] * hauteur), int(best_box[2] * largeur), int(best_box[3] * hauteur)]\n",
    "\n",
    "# dessiner les rectangles avec des coordonnées entières\n",
    "vsamlpe = Image.fromarray(im)\n",
    "draw = ImageDraw.Draw(vsamlpe)\n",
    "draw.rectangle(list(best_box_int), outline=\"red\", width=3)\n",
    "vsamlpe.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
